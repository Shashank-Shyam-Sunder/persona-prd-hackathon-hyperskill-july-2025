
# PersonaPRD â€“ AI Hackathon July 2025 ðŸš€

PersonaPRD is an AI-powered system developed during the AI Hackathon July 2025. 
It transforms unstructured Reddit feedback into structured, persona-specific 
Product Requirements Documents (PRDs) using semantic embeddings, clustering, 
and Gemini-based summarization.

ðŸŽ¯ **The goal**: Accelerate early-stage product discovery and prioritization 
by extracting pain points directly from community conversations.

## ðŸ” What It Does

- Loads persona-aligned Reddit data from curated datasets.
- Cleans and embeds posts using SBERT (`all-MiniLM-L6-v2`).
- Clusters posts with UMAP + KMeans for topic grouping.
- Computes diagnostics like silhouette and intra/inter-cluster distance ratios.
- Summarizes each cluster into user pain points using Gemini (LangChain).
- Lets you select clusters to auto-generate a structured PRD (`.docx`).

## ðŸ—‚ï¸ Project Structure

```
persona-prd-hackathon-hyperskill-july-2025/
â”‚
â”œâ”€â”€ .venv/                                 # Python virtual environment (optional)
â”œâ”€â”€ .env                                   # API credentials (GOOGLE_API_KEY, PORT)
â”œâ”€â”€ .env.example                           # Example environment variables
â”œâ”€â”€ requirements.txt                       # Python dependencies
â”œâ”€â”€ README.md                              # You're reading it
â”‚
â”œâ”€â”€ run_full_mvp_cli.py                    # ðŸ§  Run full pipeline via CLI (embedding â†’ PRD)
â”œâ”€â”€ backend/main.py                        # ðŸŒ API server implementation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ starter_datasets/
â”‚   â”‚       â”œâ”€â”€ data_neighbourhood/
â”‚   â”‚       â”‚   â”œâ”€â”€ reddit_BusinessIntelligence_hot_500.json
â”‚   â”‚       â”œâ”€â”€ vibecoding_neighbourhood/
â”‚   â”‚       â”œâ”€â”€ selfhost_neighbourhood/
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ data_neighbourhood/
â”‚       â”‚   â””â”€â”€ reddit_BusinessIntelligence_hot_500/
â”‚       â”‚       â”œâ”€â”€ clusters_KMeans_UMAP.csv
â”‚       â”‚       â”œâ”€â”€ cluster_diagnostics_metrics.csv
â”‚       â”‚       â”œâ”€â”€ pain_point_summaries.csv
â”‚       â”‚       â””â”€â”€ PRD_DRAFT.docx
â”‚       â”œâ”€â”€ vibecoding_neighbourhood/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py                        # Core pipeline logic
â”‚   â”œâ”€â”€ run_generate_prd.py               # CLI for generating PRD only
â”‚   â”œâ”€â”€ data_loader.py                    # Load Reddit JSON
â”‚   â”œâ”€â”€ preprocessing.py                  # Clean + normalize post text
â”‚   â”œâ”€â”€ embedding.py                      # Generate + cache embeddings
â”‚   â”œâ”€â”€ clustering_KMeans_UMAP.py         # UMAP + KMeans clustering
â”‚   â”œâ”€â”€ cluster_diagnostics_module.py     # Diagnostics: silhouette, intra/inter
â”‚   â”œâ”€â”€ summarization.py                  # Summarize each cluster with Gemini
â”‚   â”œâ”€â”€ prd_generator.py                  # Convert pain points to structured PRD
â”‚   â”œâ”€â”€ persona_config.py                 # Mapping between keys, display names
â”‚   â”œâ”€â”€ user_selection_utils.py           # CLI user prompts
â”‚   â””â”€â”€ utils.py                          # Save PRD as .docx
```

## ðŸš€ Quickstart

1. ðŸ”§ **Install requirements**

```bash
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
```

2. ðŸ”‘ **Set your .env**

Create a file named `.env` in the root and add your Google API key:

```env
GOOGLE_API_KEY=your-real-key-here
```
To generate Google API key,
open
https://aistudio.google.com/app/u/1/apikey?pli=1
Click on "Create API key" and follow instructions.

3. ðŸ§  **Run the full MVP pipeline**

```bash
python run_full_mvp_cli.py
```

You'll be prompted to:

- Select a persona
- Select a subreddit dataset
- Review pain point summaries
- Select which clusters to include in the PRD

## ðŸŒ API Version

In addition to the CLI version, PersonaPRD also provides a REST API that allows 
you to access the same functionality via HTTP requests.

### Setting Up the API

1. ðŸ”‘ **Update your .env file**

Make sure your `.env` file includes both the Google API key and the PORT:

```env
GOOGLE_API_KEY=your-real-key-here
PORT=8000
```

2. ðŸš€ **Run the API server**

```bash
cd backend
python main.py
```

The server will start on port 8000 
(or the port specified in your .env file).

### API Endpoints

The API provides the following endpoints:

- `GET /personas`: Get all available personas
- `GET /subreddits/{persona}`: Get subreddits for a specific persona
- `POST /run-pipeline`: Run the clustering pipeline for a selected persona 
  and subreddit
- `POST /generate-prd`: Generate a PRD for selected clusters

### API Documentation

FastAPI provides automatic interactive API documentation.
Once the server is running, you can access:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

These interfaces allow you to explore and test all available endpoints 
directly from your browser.

### API Usage Examples

#### Run Pipeline
```
POST /run-pipeline
Content-Type: application/json

{
  "persona": "vibecoding",
  "subreddit": "reddit_cursor_hot_500.json"
}
```

#### Generate PRD
```
POST /generate-prd
Content-Type: application/json

{
  "persona": "vibecoding",
  "subreddit": "reddit_cursor_hot_500.json",
  "cluster_ids": [0, 1, 4]
}
```

For more details on the API implementation, see the 
`There_is_a_console_application_performs.md` file, which describes 
the process of converting the CLI application to an API.

## ðŸ“„ Output Examples

After running the pipeline, youâ€™ll get:

- âœ… `clusters_KMeans_UMAP.csv`: Cluster labels for each post
- âœ… `cluster_diagnostics_metrics.csv`: Silhouette and distance metrics
- âœ… `pain_point_summaries.csv`: Gemini-based summaries per cluster
- âœ… `PRD_DRAFT.docx`: A fully structured PRD

## ðŸ§¾ Data Format

Each dataset is a `.json` file containing an array of Reddit posts, where each post includes:

- **Post metadata**: `id`, `title`, `selftext`, `url`, `score`, `upvote_ratio`, 
  `num_comments`, `created_utc`, `author`, `subreddit`
- **Post flags**: e.g., `is_self`, `stickied`, `locked`, `spoiler`, etc.
- **Comment threads** (up to 100): Nested objects with `id`, `body`, `score`, 
  `created_utc`, `author`, and parent relationships
- **Engagement metrics**: Vote scores, comment counts, interaction ratios

**ðŸ“Œ Current Scope in MVP:**  
We currently use only the **post title** and **selftext** for our semantic analysis. 
These are combined into a single string (`combined_text`) per post to generate embeddings.

> ðŸ’¡ Developers can experiment with incorporating other metadata 
> (e.g., comment threads, engagement) for more nuanced clustering and insights.

## ðŸ”¬ Embedding & Model Configuration

- **Embeddings**: We use `sentence-transformers/all-MiniLM-L6-v2`, producing 384-dimensional embeddings.
  - âœ… Lightweight, fast, suitable for hackathon constraints
  - ðŸ’¡ You can swap this out with higher-dimensional models like `OpenAI`'s 
    `text-embedding-3-large` for potentially better clustering and semantic results

- **LLM Summarization**: We currently use `Gemini 1.5 Flash` via LangChain 
  to summarize clusters and generate PRDs.
  - ðŸ’¡ Swap in your preferred model (e.g., GPT-4, Claude) for improved quality 
    or domain-specific tuning.

## ðŸ’¡ Ideal Use Cases

- Product Managers and Designers doing early-stage user discovery
- AI teams building semantic feedback clustering tools
- Startups validating ideas using community pain points

## ðŸ§  Credits

Built by Team PersonaPRD as part of the Hyperskill AI Engineer Bootcamp + 
Hackathon (July 2025).

## ðŸ“œ License

MIT License â€” feel free to fork, use, and build on top of this.
